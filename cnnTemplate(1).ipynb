{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethno\n",
    "\n",
    "This notebook is a global template for loading and preprocess data.\n",
    "\n",
    "## What is the SVM algorithm?\n",
    "\n",
    " It is a supervised learning algorithm designed to solve discrimination and regression problems.\n",
    " \n",
    " It is a very good image classification algorithm.\n",
    " \n",
    " ![enter image description here](https://editor.analyticsvidhya.com/uploads/61706svm3.png)\n",
    " \n",
    " ## Summary\n",
    " \n",
    " 1. [Data preparation](#prepaData)\n",
    " 2. [Learning of model](#model)\n",
    " 3. [Displaying metrics](#metric)\n",
    " 4. [Conclusion](#conclusion)\n",
    "\n",
    "### 1. Data preparation <a id=\"prepaData\"></a>\n",
    " \n",
    " - To start, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Datasets : Download the CUB datasets at this address https://deepai.org/dataset/cub-200-2011 then rename it to 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 12\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Input data files are available in the read-only \"../input/\" directory\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshutil\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import pathlib\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, Flatten, Dropout, MaxPooling2D\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_images_folder = 'images'\n",
    "if os.path.exists('./data/datasets'):\n",
    "  shutil.rmtree('./data/datasets')\n",
    "else:\n",
    "  new_images_folder = 'datasets'\n",
    "\n",
    "  image_fnames = pd.read_csv(filepath_or_buffer=os.path.join(ROOT_DIR, 'images.txt'),\n",
    "                           header=None,\n",
    "                           delimiter=' ',\n",
    "                           nrows=543,\n",
    "                           names=['Img ID', 'file path'])\n",
    "\n",
    "  image_fnames['is training image?'] = pd.read_csv(filepath_or_buffer=os.path.join(ROOT_DIR, 'train_test_split.txt'),\n",
    "                                                 header=None, delimiter=' ', nrows=543,\n",
    "                                                 names=['Img ID', 'is training image?'])['is training image?']\n",
    "  image_fnames.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./data/datasetsSelect'):\n",
    "   shutil.rmtree('./data/datasetsSelect')\n",
    "\n",
    "data_dir = os.path.join(ROOT_DIR, orig_images_folder)\n",
    "new_data_dir = os.path.join(ROOT_DIR, new_images_folder)\n",
    "os.makedirs(os.path.join(new_data_dir, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(new_data_dir, 'test'), exist_ok=True)\n",
    "os.makedirs(os.path.join(new_data_dir, 'val'), exist_ok=True)\n",
    "\n",
    "for i_image, image_fname in enumerate(image_fnames['file path']):\n",
    "    if image_fnames['is training image?'].iloc[i_image]:\n",
    "        new_dir = os.path.join(new_data_dir, 'train',\n",
    "                               image_fname.split('/')[0])\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        shutil.copy(src=os.path.join(data_dir, image_fname),\n",
    "                    dst=os.path.join(new_dir, image_fname.split('/')[1]))\n",
    "    else:\n",
    "        new_dir = os.path.join(new_data_dir, 'test', image_fname.split('/')[0])\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        shutil.copy(src=os.path.join(data_dir, image_fname),\n",
    "                    dst=os.path.join(new_dir, image_fname.split('/')[1]))\n",
    "        \n",
    "        new_dir = os.path.join(new_data_dir, 'val', image_fname.split('/')[0])\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        shutil.copy(src=os.path.join(data_dir, image_fname),\n",
    "                    dst=os.path.join(new_dir, image_fname.split('/')[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_test_pigeon = './data/datasets/test/058.Pigeon_Guillemot'\n",
    "# dest_test_pigeon = './data/datasetsSelect/test/058.Pigeon_Guillemot'\n",
    "\n",
    "# src_train_pigeon = './data/datasets/train/058.Pigeon_Guillemot'\n",
    "# dest_train_pigeon = './data/datasetsSelect/train/058.Pigeon_Guillemot'\n",
    "\n",
    "# src_val_pigeon = './data/datasets/val/058.Pigeon_Guillemot'\n",
    "# dest_val_pigeon = './data/datasetsSelect/val/058.Pigeon_Guillemot'\n",
    "\n",
    "# src_test_woodpecker = './data/datasets/test/191.Red_headed_Woodpecker'\n",
    "# dest_test_woodpecker = './data/datasetsSelect/test/191.Red_headed_Woodpecker'\n",
    "\n",
    "# src_val_woodpecker = './data/datasets/val/191.Red_headed_Woodpecker'\n",
    "# dest_val_woodpecker = './data/datasetsSelect/val/191.Red_headed_Woodpecker'\n",
    "\n",
    "# src_train_woodpecker = './data/datasets/train/191.Red_headed_Woodpecker'\n",
    "# dest_train_woodpecker = './data/datasetsSelect/train/191.Red_headed_Woodpecker'\n",
    "\n",
    "src_test_footed_albatross = './data/datasets/test/001.Black_footed_Albatross'\n",
    "dest_test_footed_albatross = './data/datasetsSelect/test/001.Black_footed_Albatross'\n",
    "\n",
    "src_train_footed_albatross = './data/datasets/train/001.Black_footed_Albatross'\n",
    "dest_train_footed_albatross = './data/datasetsSelect/train/001.Black_footed_Albatross'\n",
    "\n",
    "src_val_footed_albatross = './data/datasets/val/001.Black_footed_Albatross'\n",
    "dest_val_footed_albatross = './data/datasetsSelect/val/001.Black_footed_Albatross'\n",
    "\n",
    "\n",
    "src_test_laysan_albatross = './data/datasets/test/002.Laysan_Albatross'\n",
    "dest_test_laysan_albatross = './data/datasetsSelect/test/002.Laysan_Albatross'\n",
    "\n",
    "src_val_laysan_albatross = './data/datasets/val/002.Laysan_Albatross'\n",
    "dest_val_laysan_albatross = './data/datasetsSelect/val/002.Laysan_Albatross'\n",
    "\n",
    "src_train_laysan_albatross = './data/datasets/train/002.Laysan_Albatross'\n",
    "dest_train_laysan_albatross = './data/datasetsSelect/train/002.Laysan_Albatross'\n",
    "\n",
    "\n",
    "src_test_sooty_albatross = './data/datasets/test/003.Sooty_Albatross'\n",
    "dest_test_sooty_albatross = './data/datasetsSelect/test/003.Sooty_Albatross'\n",
    "\n",
    "src_val_sooty_albatross = './data/datasets/val/003.Sooty_Albatross'\n",
    "dest_val_sooty_albatross = './data/datasetsSelect/val/003.Sooty_Albatross'\n",
    "\n",
    "src_train_sooty_albatross = './data/datasets/train/003.Sooty_Albatross'\n",
    "dest_train_sooty_albatross = './data/datasetsSelect/train/003.Sooty_Albatross'\n",
    "\n",
    "\n",
    "src_test_groove_billed_ani = './data/datasets/test/004.Groove_billed_Ani'\n",
    "dest_test_groove_billed_ani = './data/datasetsSelect/test/004.Groove_billed_Ani'\n",
    "\n",
    "src_val_groove_billed_ani = './data/datasets/val/004.Groove_billed_Ani'\n",
    "dest_val_groove_billed_ani = './data/datasetsSelect/val/004.Groove_billed_Ani'\n",
    "\n",
    "src_train_groove_billed_ani = './data/datasets/train/004.Groove_billed_Ani'\n",
    "dest_train_groove_billed_ani = './data/datasetsSelect/train/004.Groove_billed_Ani'\n",
    "\n",
    "\n",
    "src_test_crested_auklet = './data/datasets/test/005.Crested_Auklet'\n",
    "dest_test_crested_auklet = './data/datasetsSelect/test/005.Crested_Auklet'\n",
    "\n",
    "src_val_crested_auklet = './data/datasets/val/005.Crested_Auklet'\n",
    "dest_val_crested_auklet = './data/datasetsSelect/val/005.Crested_Auklet'\n",
    "\n",
    "src_train_crested_auklet = './data/datasets/train/005.Crested_Auklet'\n",
    "dest_train_crested_auklet = './data/datasetsSelect/train/005.Crested_Auklet'\n",
    "\n",
    "\n",
    "src_test_least_auklet = './data/datasets/test/006.Least_Auklet'\n",
    "dest_test_least_auklet = './data/datasetsSelect/test/006.Least_Auklet'\n",
    "\n",
    "src_val_least_auklet = './data/datasets/val/006.Least_Auklet'\n",
    "dest_val_least_auklet = './data/datasetsSelect/val/006.Least_Auklet'\n",
    "\n",
    "src_train_least_auklet = './data/datasets/train/006.Least_Auklet'\n",
    "dest_train_least_auklet = './data/datasetsSelect/train/006.Least_Auklet'\n",
    "\n",
    "\n",
    "src_test_parakeet_auklet = './data/datasets/test/007.Parakeet_Auklet'\n",
    "dest_test_parakeet_auklet = './data/datasetsSelect/test/007.Parakeet_Auklet'\n",
    "\n",
    "src_val_parakeet_auklet = './data/datasets/val/007.Parakeet_Auklet'\n",
    "dest_val_parakeet_auklet = './data/datasetsSelect/val/007.Parakeet_Auklet'\n",
    "\n",
    "src_train_parakeet_auklet = './data/datasets/train/007.Parakeet_Auklet'\n",
    "dest_train_parakeet_auklet = './data/datasetsSelect/train/007.Parakeet_Auklet'\n",
    "\n",
    "\n",
    "src_test_rhinoceros_auklet = './data/datasets/test/008.Rhinoceros_Auklet'\n",
    "dest_test_rhinoceros_auklet = './data/datasetsSelect/test/008.Rhinoceros_Auklet'\n",
    "\n",
    "src_val_rhinoceros_auklet = './data/datasets/val/008.Rhinoceros_Auklet'\n",
    "dest_val_rhinoceros_auklet = './data/datasetsSelect/val/008.Rhinoceros_Auklet'\n",
    "\n",
    "src_train_rhinoceros_auklet = './data/datasets/train/008.Rhinoceros_Auklet'\n",
    "dest_train_rhinoceros_auklet = './data/datasetsSelect/train/008.Rhinoceros_Auklet'\n",
    "\n",
    "\n",
    "src_test_brewer_blackbird = './data/datasets/test/009.Brewer_Blackbird'\n",
    "dest_test_brewer_blackbird = './data/datasetsSelect/test/009.Brewer_Blackbird'\n",
    "\n",
    "src_val_brewer_blackbird = './data/datasets/val/009.Brewer_Blackbird'\n",
    "dest_val_brewer_blackbird = './data/datasetsSelect/val/009.Brewer_Blackbird'\n",
    "\n",
    "src_train_brewer_blackbird = './data/datasets/train/009.Brewer_Blackbird'\n",
    "dest_train_brewer_blackbird = './data/datasetsSelect/train/009.Brewer_Blackbird'\n",
    "\n",
    "\n",
    "src_test_red_winged_blackbird = './data/datasets/test/010.Red_winged_Blackbird'\n",
    "dest_test_red_winged_blackbird = './data/datasetsSelect/test/010.Red_winged_Blackbird'\n",
    "\n",
    "src_val_red_winged_blackbird = './data/datasets/val/010.Red_winged_Blackbird'\n",
    "dest_val_red_winged_blackbird = './data/datasetsSelect/val/010.Red_winged_Blackbird'\n",
    "\n",
    "src_train_red_winged_blackbird = './data/datasets/train/010.Red_winged_Blackbird'\n",
    "dest_train_red_winged_blackbird = './data/datasetsSelect/train/010.Red_winged_Blackbird'\n",
    "\n",
    "\n",
    "shutil.move(src_test_footed_albatross, dest_test_footed_albatross)\n",
    "shutil.move(src_train_footed_albatross, dest_train_footed_albatross)\n",
    "shutil.move(src_val_footed_albatross, dest_val_footed_albatross)\n",
    "\n",
    "shutil.move(src_test_laysan_albatross, dest_test_laysan_albatross)\n",
    "shutil.move(src_train_laysan_albatross, dest_train_laysan_albatross)\n",
    "shutil.move(src_val_laysan_albatross, dest_val_laysan_albatross)\n",
    "\n",
    "shutil.move(src_test_sooty_albatross, dest_test_sooty_albatross)\n",
    "shutil.move(src_train_sooty_albatross, dest_train_sooty_albatross)\n",
    "shutil.move(src_val_sooty_albatross, dest_val_sooty_albatross)\n",
    "\n",
    "shutil.move(src_test_groove_billed_ani, dest_test_groove_billed_ani)\n",
    "shutil.move(src_train_groove_billed_ani, dest_train_groove_billed_ani)\n",
    "shutil.move(src_val_groove_billed_ani, dest_val_groove_billed_ani)\n",
    "\n",
    "shutil.move(src_test_crested_auklet, dest_test_crested_auklet)\n",
    "shutil.move(src_train_crested_auklet, dest_train_crested_auklet)\n",
    "shutil.move(src_val_crested_auklet, dest_val_crested_auklet)\n",
    "\n",
    "\n",
    "shutil.move(src_test_least_auklet, dest_test_least_auklet)\n",
    "shutil.move(src_train_least_auklet, dest_train_least_auklet)\n",
    "shutil.move(src_val_least_auklet, dest_val_least_auklet)\n",
    "\n",
    "shutil.move(src_val_parakeet_auklet, dest_val_parakeet_auklet)\n",
    "shutil.move(src_test_parakeet_auklet, dest_test_parakeet_auklet)\n",
    "shutil.move(src_train_parakeet_auklet, dest_train_parakeet_auklet)\n",
    "\n",
    "shutil.move(src_val_rhinoceros_auklet, dest_val_rhinoceros_auklet)\n",
    "shutil.move(src_test_rhinoceros_auklet, dest_test_rhinoceros_auklet)\n",
    "shutil.move(src_train_rhinoceros_auklet, dest_train_rhinoceros_auklet)\n",
    "\n",
    "\n",
    "shutil.move(src_val_brewer_blackbird, dest_val_brewer_blackbird)\n",
    "shutil.move(src_test_brewer_blackbird, dest_test_brewer_blackbird)\n",
    "shutil.move(src_train_brewer_blackbird, dest_train_brewer_blackbird)\n",
    "\n",
    "\n",
    "shutil.move(src_val_red_winged_blackbird, dest_val_red_winged_blackbird)\n",
    "shutil.move(src_test_red_winged_blackbird, dest_test_red_winged_blackbird)\n",
    "shutil.move(src_train_red_winged_blackbird, dest_train_red_winged_blackbird)\n",
    "\n",
    "\n",
    "shutil.rmtree('./data/datasets')\n",
    "\n",
    "print('prepare dataset structure done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We define all necessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/datasetsSelect'\n",
    "train_path = './data/datasetsSelect/train'\n",
    "test_path = './data/datasetsSelect/test'\n",
    "val_path = './data/datasetsSelect/val'\n",
    "IMG_SIZE = 100\n",
    "scale = 1\n",
    "batch_size = 10\n",
    "nb_train_samples = 100\n",
    "nb_validation_samples = 100\n",
    "epochs = 10\n",
    "labels = pd.read_csv(\"./data/classes.txt\", nrows=10, names=[\"Classes\"])\n",
    "\n",
    "\n",
    "print(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(data_dir, size):\n",
    "    data = list()\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for image in os.listdir(path):\n",
    "            if(image != \".DS_Store\"):\n",
    "                try:\n",
    "                    image_arr = cv2.imread(os.path.join(\n",
    "                        path, image), cv2.IMREAD_GRAYSCALE)\n",
    "                    resized_arr = cv2.resize(image_arr, (size, size))\n",
    "                    data.append([resized_arr, class_num])\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "    return np.array(data, dtype=\"object\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_training_data(test_path, IMG_SIZE)\n",
    "train = get_training_data(train_path, IMG_SIZE)\n",
    "val = get_training_data(val_path, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_datasets(array):\n",
    "    arr_1 = array[np.where(array[:, 1] == 1)]\n",
    "    arr_0 = array[np.where(array[:, 1] == 0)]\n",
    "    l_arr_1 = len(arr_1)\n",
    "    l_arr_0 = len(arr_0)\n",
    "    diff = 0\n",
    "\n",
    "    if l_arr_0 > l_arr_1:\n",
    "        diff = l_arr_0 - l_arr_1\n",
    "        arr_0.resize((l_arr_0 - diff, arr_0.shape[1]))\n",
    "        array = np.concatenate((l_arr_0, arr_1))\n",
    "    else:\n",
    "        diff = l_arr_1 - l_arr_0\n",
    "        arr_1.resize((l_arr_1 - diff, arr_1.shape[1]))\n",
    "        array = np.concatenate((arr_0, arr_1))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = train[np.random.randint(\n",
    "    train.shape[0], size=int(len(train)*scale)), :]\n",
    "test_scaled = test[np.random.randint(\n",
    "    test.shape[0], size=int(len(test)*scale)), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "    X_ = list()\n",
    "    y_ = list()\n",
    "    for features, label in dataset:\n",
    "        X_.append(features)\n",
    "        y_.append(label)\n",
    "    X_ = np.array(X_)\n",
    "    X_ = X_.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    y_ = np.array(y_)\n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, label_train = prepare_dataset(mix_datasets(train))\n",
    "x_test, label_test = prepare_dataset(mix_datasets(test))\n",
    "x_val, label_val = prepare_dataset(mix_datasets(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append train + test + val data in order to scale our datasets\n",
    "X = np.concatenate((x_train, x_test))\n",
    "X = np.concatenate((X, x_val))\n",
    "Y = np.concatenate((label_train, label_test))\n",
    "Y = np.concatenate((Y, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-Out cross validation\n",
    "# test_size: what proportion of original data is used for test set \n",
    "x_train, x_test, label_train, label_test = train_test_split( X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = np.array(x_train).max()\n",
    "x_train = x_train / xmax\n",
    "xmax = np.array(x_test).max()\n",
    "x_test = x_test / xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Learning of model <a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "          input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120))\n",
    "model.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init training's Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbacks():\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='lstmchar256256128test.h5',\n",
    "            monitor='loss',\n",
    "            save_best_only=True\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='loss',\n",
    "            min_delta=0,\n",
    "            patience=4,\n",
    "            mode='auto',\n",
    "            baseline=None,\n",
    "        )\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(label_test)\n",
    "y_train = np.array(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "time1 = time.time()\n",
    "history = model.fit(x_train, y_train, epochs=20, validation_data=(\n",
    "    x_test, y_test), callbacks=callbacks())\n",
    "\n",
    "# Print duration time\n",
    "duration_time = time.time() - time1\n",
    "if duration_time < 60:\n",
    "    print(\"{} s\".format(duration_time))\n",
    "else:\n",
    "    print(\"{} min\".format(duration_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Displaying metrics <a id=\"metric\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss + accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_loss = history.history[\"loss\"]\n",
    "history_acc = history.history[\"accuracy\"]\n",
    "\n",
    "plt.plot(history_loss, label=\"loss\")\n",
    "plt.plot(history_acc, label=\"acc\")\n",
    "plt.title(\"Training history\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(pred, axis=1)\n",
    "report = classification_report(predicted_classes, y_test, target_names=labels)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model obtains 85% precision for the guillemot pigeon against 81% for the Red headed woodpecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cunsion Mactix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test loss     : {score[0]:4.4f}')\n",
    "print(f'Test accuracy : {score[1]:4.4f}')\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion <a id=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the confusion matrix, we obtain an accuracy of 82% for the guillemot pigeon with a few proportion of false negatives , as well as an accuracy of 84% for the Red headed woodpecker with a small amount of false positive.\n",
    "\n",
    "The cause of this discrepancy in accuracy can be due to several things such as an uneven distribution of our starting dataset with one class more present than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('model.sav', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "144f974373c9fda24aaee5df054dd1f07b7d95d6200df4c599d8b429227f40a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
